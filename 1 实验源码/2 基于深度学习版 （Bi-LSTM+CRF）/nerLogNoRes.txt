Some weights of BertModel were not initialized from the model checkpoint at ./myBert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-06-23 21:58:58,670 DEBUG    bert.embeddings.word_embeddings.weight: torch.Size([21421, 768]), require_grad=False
2024-06-23 21:58:58,670 DEBUG    bert.embeddings.position_embeddings.weight: torch.Size([512, 768]), require_grad=False
2024-06-23 21:58:58,670 DEBUG    bert.embeddings.token_type_embeddings.weight: torch.Size([2, 768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.embeddings.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.embeddings.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,671 DEBUG    bert.encoder.layer.0.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.1.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.2.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.2.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,672 DEBUG    bert.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.2.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.3.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.3.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.3.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.3.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,673 DEBUG    bert.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.3.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.3.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.3.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.3.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.3.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,674 DEBUG    bert.encoder.layer.4.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.4.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.4.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.5.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,675 DEBUG    bert.encoder.layer.6.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.6.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.7.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.7.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,676 DEBUG    bert.encoder.layer.7.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.7.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.7.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.7.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.7.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.7.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.7.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,677 DEBUG    bert.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.8.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.8.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.8.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.8.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.8.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,678 DEBUG    bert.encoder.layer.9.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.10.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768]), require_grad=True
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.11.attention.self.query.bias: torch.Size([768]), require_grad=True
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768]), require_grad=True
2024-06-23 21:58:58,679 DEBUG    bert.encoder.layer.11.attention.self.key.bias: torch.Size([768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.attention.self.value.bias: torch.Size([768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.attention.output.dense.bias: torch.Size([768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.intermediate.dense.bias: torch.Size([3072]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.output.dense.weight: torch.Size([768, 3072]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.output.dense.bias: torch.Size([768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.output.LayerNorm.weight: torch.Size([768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.encoder.layer.11.output.LayerNorm.bias: torch.Size([768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.pooler.dense.weight: torch.Size([768, 768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    bert.pooler.dense.bias: torch.Size([768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    lstm.weight_ih_l0: torch.Size([512, 768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    lstm.weight_hh_l0: torch.Size([512, 128]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    lstm.bias_ih_l0: torch.Size([512]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    lstm.bias_hh_l0: torch.Size([512]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    lstm.weight_ih_l0_reverse: torch.Size([512, 768]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    lstm.weight_hh_l0_reverse: torch.Size([512, 128]), require_grad=True
2024-06-23 21:58:58,680 DEBUG    lstm.bias_ih_l0_reverse: torch.Size([512]), require_grad=True
2024-06-23 21:58:58,681 DEBUG    lstm.bias_hh_l0_reverse: torch.Size([512]), require_grad=True
2024-06-23 21:58:58,681 DEBUG    residual_projection.weight: torch.Size([256, 256]), require_grad=True
2024-06-23 21:58:58,681 DEBUG    residual_projection.bias: torch.Size([256]), require_grad=True
2024-06-23 21:58:58,681 DEBUG    BN.weight: torch.Size([256]), require_grad=True
2024-06-23 21:58:58,681 DEBUG    BN.bias: torch.Size([256]), require_grad=True
2024-06-23 21:58:58,681 DEBUG    hidden2tag.weight: torch.Size([21, 256]), require_grad=True
2024-06-23 21:58:58,681 DEBUG    hidden2tag.bias: torch.Size([21]), require_grad=True
2024-06-23 21:58:58,681 DEBUG    crf.start_transitions: torch.Size([21]), require_grad=True
2024-06-23 21:58:58,681 DEBUG    crf.end_transitions: torch.Size([21]), require_grad=True
2024-06-23 21:58:58,681 DEBUG    crf.transitions: torch.Size([21, 21]), require_grad=True
/home/vipuser/miniconda3/envs/nlplab/lib/python3.10/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
2024-06-23 21:59:32,286 DEBUG    epoch 0-step 100 loss: 29.554912
2024-06-23 22:00:04,893 DEBUG    epoch 0-step 200 loss: 13.382366
2024-06-23 22:00:38,576 DEBUG    epoch 0-step 300 loss: 9.100096
2024-06-23 22:01:12,861 DEBUG    epoch 0-step 400 loss: 7.065956
2024-06-23 22:01:45,858 DEBUG    epoch 0-step 500 loss: 5.953813
2024-06-23 22:02:22,210 DEBUG    epoch 0-step 600 loss: 5.356931
2024-06-23 22:02:35,337 INFO     precision: 0.888172
2024-06-23 22:02:35,337 INFO     recall: 0.797682
2024-06-23 22:02:35,337 INFO     fscore: 0.840499
2024-06-23 22:03:11,943 DEBUG    epoch 1-step 100 loss: 5.803026
2024-06-23 22:03:48,667 DEBUG    epoch 1-step 200 loss: 5.564631
2024-06-23 22:04:24,253 DEBUG    epoch 1-step 300 loss: 5.245499
2024-06-23 22:04:59,549 DEBUG    epoch 1-step 400 loss: 4.982089
2024-06-23 22:05:35,678 DEBUG    epoch 1-step 500 loss: 4.883875
2024-06-23 22:06:11,593 DEBUG    epoch 1-step 600 loss: 4.378202
2024-06-23 22:06:27,891 INFO     precision: 0.900851
2024-06-23 22:06:27,891 INFO     recall: 0.868662
2024-06-23 22:06:27,891 INFO     fscore: 0.884464
2024-06-23 22:07:12,718 DEBUG    epoch 2-step 100 loss: 4.175743
2024-06-23 22:08:02,153 DEBUG    epoch 2-step 200 loss: 3.994888
2024-06-23 22:08:52,184 DEBUG    epoch 2-step 300 loss: 3.958160
2024-06-23 22:09:40,550 DEBUG    epoch 2-step 400 loss: 3.481274
2024-06-23 22:10:29,622 DEBUG    epoch 2-step 500 loss: 3.579347
2024-06-23 22:11:18,741 DEBUG    epoch 2-step 600 loss: 3.475323
2024-06-23 22:11:34,122 INFO     precision: 0.905825
2024-06-23 22:11:34,123 INFO     recall: 0.901014
2024-06-23 22:11:34,123 INFO     fscore: 0.903413
2024-06-23 22:12:26,659 DEBUG    epoch 3-step 100 loss: 3.296043
2024-06-23 22:13:20,049 DEBUG    epoch 3-step 200 loss: 3.205893
2024-06-23 22:14:06,297 DEBUG    epoch 3-step 300 loss: 3.039858
2024-06-23 22:14:56,199 DEBUG    epoch 3-step 400 loss: 3.113776
2024-06-23 22:15:44,374 DEBUG    epoch 3-step 500 loss: 3.119740
2024-06-23 22:16:33,113 DEBUG    epoch 3-step 600 loss: 2.905600
2024-06-23 22:16:48,885 INFO     precision: 0.916907
2024-06-23 22:16:48,886 INFO     recall: 0.921777
2024-06-23 22:16:48,886 INFO     fscore: 0.919335
2024-06-23 22:17:40,889 DEBUG    epoch 4-step 100 loss: 2.676778
2024-06-23 22:18:15,757 DEBUG    epoch 4-step 200 loss: 2.667100
2024-06-23 22:18:54,434 DEBUG    epoch 4-step 300 loss: 2.642916
2024-06-23 22:19:44,976 DEBUG    epoch 4-step 400 loss: 2.573166
2024-06-23 22:20:32,925 DEBUG    epoch 4-step 500 loss: 2.634816
2024-06-23 22:21:25,981 DEBUG    epoch 4-step 600 loss: 2.494423
2024-06-23 22:21:42,757 INFO     precision: 0.923482
2024-06-23 22:21:42,758 INFO     recall: 0.932400
2024-06-23 22:21:42,758 INFO     fscore: 0.927919
2024-06-23 22:22:33,644 DEBUG    epoch 5-step 100 loss: 2.352876
2024-06-23 22:23:23,064 DEBUG    epoch 5-step 200 loss: 2.295761
2024-06-23 22:24:14,649 DEBUG    epoch 5-step 300 loss: 2.363405
2024-06-23 22:25:08,867 DEBUG    epoch 5-step 400 loss: 2.262422
2024-06-23 22:25:57,759 DEBUG    epoch 5-step 500 loss: 2.267647
2024-06-23 22:26:47,039 DEBUG    epoch 5-step 600 loss: 2.172246
2024-06-23 22:27:03,389 INFO     precision: 0.931018
2024-06-23 22:27:03,390 INFO     recall: 0.931917
2024-06-23 22:27:03,390 INFO     fscore: 0.931467
2024-06-23 22:27:53,572 DEBUG    epoch 6-step 100 loss: 2.015859
2024-06-23 22:28:43,373 DEBUG    epoch 6-step 200 loss: 2.035214
2024-06-23 22:29:36,493 DEBUG    epoch 6-step 300 loss: 1.992053
2024-06-23 22:30:28,107 DEBUG    epoch 6-step 400 loss: 2.020003
2024-06-23 22:31:18,585 DEBUG    epoch 6-step 500 loss: 1.936698
2024-06-23 22:32:11,143 DEBUG    epoch 6-step 600 loss: 1.945205
2024-06-23 22:32:26,713 INFO     precision: 0.943925
2024-06-23 22:32:26,713 INFO     recall: 0.926606
2024-06-23 22:32:26,713 INFO     fscore: 0.935185
2024-06-23 22:33:14,417 DEBUG    epoch 7-step 100 loss: 1.779954
2024-06-23 22:34:07,417 DEBUG    epoch 7-step 200 loss: 1.763334
2024-06-23 22:34:54,797 DEBUG    epoch 7-step 300 loss: 1.783014
2024-06-23 22:35:46,533 DEBUG    epoch 7-step 400 loss: 1.722024
2024-06-23 22:36:36,755 DEBUG    epoch 7-step 500 loss: 1.719262
2024-06-23 22:37:27,310 DEBUG    epoch 7-step 600 loss: 1.735724
2024-06-23 22:37:46,273 INFO     precision: 0.940891
2024-06-23 22:37:46,273 INFO     recall: 0.937711
2024-06-23 22:37:46,273 INFO     fscore: 0.939299
2024-06-23 22:38:40,736 DEBUG    epoch 8-step 100 loss: 1.601900
2024-06-23 22:39:29,425 DEBUG    epoch 8-step 200 loss: 1.564542
2024-06-23 22:40:18,642 DEBUG    epoch 8-step 300 loss: 1.558016
2024-06-23 22:41:05,792 DEBUG    epoch 8-step 400 loss: 1.689486
2024-06-23 22:41:55,210 DEBUG    epoch 8-step 500 loss: 1.553636
2024-06-23 22:42:46,529 DEBUG    epoch 8-step 600 loss: 1.576661
2024-06-23 22:43:02,520 INFO     precision: 0.943050
2024-06-23 22:43:02,520 INFO     recall: 0.943506
2024-06-23 22:43:02,521 INFO     fscore: 0.943278
2024-06-23 22:43:54,771 DEBUG    epoch 9-step 100 loss: 1.480254
2024-06-23 22:44:45,355 DEBUG    epoch 9-step 200 loss: 1.435931
2024-06-23 22:45:35,587 DEBUG    epoch 9-step 300 loss: 1.348349
2024-06-23 22:46:25,363 DEBUG    epoch 9-step 400 loss: 1.410594
2024-06-23 22:47:20,300 DEBUG    epoch 9-step 500 loss: 1.348937
2024-06-23 22:48:08,448 DEBUG    epoch 9-step 600 loss: 1.415657
2024-06-23 22:48:24,950 INFO     precision: 0.926679
2024-06-23 22:48:24,950 INFO     recall: 0.945920
2024-06-23 22:48:24,950 INFO     fscore: 0.936201
2024-06-23 22:48:25,916 INFO     model has been saved in  ./nerSave/model_epoch9.pkl
2024-06-23 22:49:15,396 DEBUG    epoch 10-step 100 loss: 1.240115
2024-06-23 22:50:07,240 DEBUG    epoch 10-step 200 loss: 1.249627
2024-06-23 22:51:03,416 DEBUG    epoch 10-step 300 loss: 1.228214
2024-06-23 22:51:50,926 DEBUG    epoch 10-step 400 loss: 1.398419
2024-06-23 22:52:45,055 DEBUG    epoch 10-step 500 loss: 1.266291
2024-06-23 22:53:35,831 DEBUG    epoch 10-step 600 loss: 1.253428
2024-06-23 22:53:52,963 INFO     precision: 0.943078
2024-06-23 22:53:52,964 INFO     recall: 0.943988
2024-06-23 22:53:52,964 INFO     fscore: 0.943533
2024-06-23 22:54:42,211 DEBUG    epoch 11-step 100 loss: 1.167123
2024-06-23 22:55:33,457 DEBUG    epoch 11-step 200 loss: 1.146935
2024-06-23 22:56:23,863 DEBUG    epoch 11-step 300 loss: 1.207464
2024-06-23 22:57:11,593 DEBUG    epoch 11-step 400 loss: 1.136887
2024-06-23 22:58:03,536 DEBUG    epoch 11-step 500 loss: 1.185788
2024-06-23 22:58:55,611 DEBUG    epoch 11-step 600 loss: 1.157607
2024-06-23 22:59:14,792 INFO     precision: 0.948027
2024-06-23 22:59:14,793 INFO     recall: 0.951231
2024-06-23 22:59:14,793 INFO     fscore: 0.949626
2024-06-23 23:00:08,543 DEBUG    epoch 12-step 100 loss: 1.013574
2024-06-23 23:00:56,743 DEBUG    epoch 12-step 200 loss: 1.051363
2024-06-23 23:01:45,683 DEBUG    epoch 12-step 300 loss: 1.087873
2024-06-23 23:02:37,143 DEBUG    epoch 12-step 400 loss: 1.103444
2024-06-23 23:03:27,421 DEBUG    epoch 12-step 500 loss: 1.108259
2024-06-23 23:04:18,748 DEBUG    epoch 12-step 600 loss: 1.091173
2024-06-23 23:04:34,302 INFO     precision: 0.938184
2024-06-23 23:04:34,302 INFO     recall: 0.952680
2024-06-23 23:04:34,303 INFO     fscore: 0.945376
2024-06-23 23:05:26,813 DEBUG    epoch 13-step 100 loss: 0.984478
2024-06-23 23:06:16,976 DEBUG    epoch 13-step 200 loss: 0.983408
2024-06-23 23:07:08,382 DEBUG    epoch 13-step 300 loss: 0.952954
2024-06-23 23:08:02,213 DEBUG    epoch 13-step 400 loss: 0.973641
2024-06-23 23:08:54,924 DEBUG    epoch 13-step 500 loss: 0.989620
2024-06-23 23:09:44,894 DEBUG    epoch 13-step 600 loss: 0.999185
2024-06-23 23:10:00,074 INFO     precision: 0.932171
2024-06-23 23:10:00,074 INFO     recall: 0.955577
2024-06-23 23:10:00,074 INFO     fscore: 0.943729
2024-06-23 23:10:50,329 DEBUG    epoch 14-step 100 loss: 0.829040
2024-06-23 23:11:39,997 DEBUG    epoch 14-step 200 loss: 0.821585
2024-06-23 23:12:33,568 DEBUG    epoch 14-step 300 loss: 0.907140
2024-06-23 23:13:21,707 DEBUG    epoch 14-step 400 loss: 0.886909
2024-06-23 23:14:11,187 DEBUG    epoch 14-step 500 loss: 0.924780
2024-06-23 23:15:03,882 DEBUG    epoch 14-step 600 loss: 0.914975
2024-06-23 23:15:18,868 INFO     precision: 0.944444
2024-06-23 23:15:18,868 INFO     recall: 0.960406
2024-06-23 23:15:18,868 INFO     fscore: 0.952358
2024-06-23 23:16:09,496 DEBUG    epoch 15-step 100 loss: 0.815432
2024-06-23 23:16:59,673 DEBUG    epoch 15-step 200 loss: 0.826958
2024-06-23 23:17:49,496 DEBUG    epoch 15-step 300 loss: 0.864455
2024-06-23 23:18:39,090 DEBUG    epoch 15-step 400 loss: 0.831839
2024-06-23 23:19:32,709 DEBUG    epoch 15-step 500 loss: 0.802564
2024-06-23 23:20:25,380 DEBUG    epoch 15-step 600 loss: 0.804479
2024-06-23 23:20:41,402 INFO     precision: 0.949112
2024-06-23 23:20:41,404 INFO     recall: 0.954611
2024-06-23 23:20:41,404 INFO     fscore: 0.951854
2024-06-23 23:21:31,629 DEBUG    epoch 16-step 100 loss: 0.760678
2024-06-23 23:22:24,539 DEBUG    epoch 16-step 200 loss: 0.751357
2024-06-23 23:23:15,925 DEBUG    epoch 16-step 300 loss: 0.738712
2024-06-23 23:24:06,641 DEBUG    epoch 16-step 400 loss: 0.744596
2024-06-23 23:24:58,663 DEBUG    epoch 16-step 500 loss: 0.758243
2024-06-23 23:25:51,416 DEBUG    epoch 16-step 600 loss: 0.765942
2024-06-23 23:26:09,530 INFO     precision: 0.955188
2024-06-23 23:26:09,531 INFO     recall: 0.946886
2024-06-23 23:26:09,531 INFO     fscore: 0.951018
2024-06-23 23:27:00,791 DEBUG    epoch 17-step 100 loss: 0.676639
2024-06-23 23:27:54,660 DEBUG    epoch 17-step 200 loss: 0.722542
2024-06-23 23:28:47,385 DEBUG    epoch 17-step 300 loss: 0.720118
2024-06-23 23:29:39,511 DEBUG    epoch 17-step 400 loss: 0.725084
2024-06-23 23:30:29,106 DEBUG    epoch 17-step 500 loss: 0.730173
2024-06-23 23:31:18,553 DEBUG    epoch 17-step 600 loss: 0.727231
2024-06-23 23:31:34,539 INFO     precision: 0.951302
2024-06-23 23:31:34,540 INFO     recall: 0.952680
2024-06-23 23:31:34,540 INFO     fscore: 0.951990
2024-06-23 23:32:29,475 DEBUG    epoch 18-step 100 loss: 0.642146
2024-06-23 23:33:15,796 DEBUG    epoch 18-step 200 loss: 0.638822
2024-06-23 23:34:06,666 DEBUG    epoch 18-step 300 loss: 0.669423
2024-06-23 23:34:55,757 DEBUG    epoch 18-step 400 loss: 0.647460
2024-06-23 23:35:47,771 DEBUG    epoch 18-step 500 loss: 0.663930
2024-06-23 23:36:38,350 DEBUG    epoch 18-step 600 loss: 0.702116
2024-06-23 23:36:55,725 INFO     precision: 0.944791
2024-06-23 23:36:55,725 INFO     recall: 0.950266
2024-06-23 23:36:55,726 INFO     fscore: 0.947520
2024-06-23 23:37:43,796 DEBUG    epoch 19-step 100 loss: 0.609705
2024-06-23 23:38:35,505 DEBUG    epoch 19-step 200 loss: 0.622665
2024-06-23 23:39:29,479 DEBUG    epoch 19-step 300 loss: 0.622595
2024-06-23 23:40:21,408 DEBUG    epoch 19-step 400 loss: 0.612859
2024-06-23 23:41:15,833 DEBUG    epoch 19-step 500 loss: 0.621992
2024-06-23 23:42:04,979 DEBUG    epoch 19-step 600 loss: 0.597289
2024-06-23 23:42:22,694 INFO     precision: 0.948102
2024-06-23 23:42:22,695 INFO     recall: 0.952680
2024-06-23 23:42:22,695 INFO     fscore: 0.950385
2024-06-23 23:42:23,678 INFO     model has been saved in  ./nerSave/model_epoch19.pkl
2024-06-23 23:43:13,488 DEBUG    epoch 20-step 100 loss: 0.558935
2024-06-23 23:44:07,914 DEBUG    epoch 20-step 200 loss: 0.584291
2024-06-23 23:45:00,417 DEBUG    epoch 20-step 300 loss: 0.538886
2024-06-23 23:45:50,468 DEBUG    epoch 20-step 400 loss: 0.524631
2024-06-23 23:46:40,988 DEBUG    epoch 20-step 500 loss: 0.599936
2024-06-23 23:47:32,605 DEBUG    epoch 20-step 600 loss: 0.618571
2024-06-23 23:47:49,984 INFO     precision: 0.953455
2024-06-23 23:47:49,984 INFO     recall: 0.959440
2024-06-23 23:47:49,985 INFO     fscore: 0.956438
2024-06-23 23:48:40,795 DEBUG    epoch 21-step 100 loss: 0.537531
2024-06-23 23:49:32,581 DEBUG    epoch 21-step 200 loss: 0.537027
2024-06-23 23:50:23,139 DEBUG    epoch 21-step 300 loss: 0.534611
2024-06-23 23:51:12,237 DEBUG    epoch 21-step 400 loss: 0.562701
2024-06-23 23:52:06,166 DEBUG    epoch 21-step 500 loss: 0.548028
2024-06-23 23:52:54,333 DEBUG    epoch 21-step 600 loss: 0.519901
2024-06-23 23:53:11,135 INFO     precision: 0.957219
2024-06-23 23:53:11,136 INFO     recall: 0.950748
2024-06-23 23:53:11,136 INFO     fscore: 0.953973
2024-06-23 23:54:02,037 DEBUG    epoch 22-step 100 loss: 0.518287
2024-06-23 23:54:52,261 DEBUG    epoch 22-step 200 loss: 0.480158
2024-06-23 23:55:45,061 DEBUG    epoch 22-step 300 loss: 0.478357
2024-06-23 23:56:37,972 DEBUG    epoch 22-step 400 loss: 0.514087
2024-06-23 23:57:28,458 DEBUG    epoch 22-step 500 loss: 0.526327
2024-06-23 23:58:17,355 DEBUG    epoch 22-step 600 loss: 0.491066
2024-06-23 23:58:33,534 INFO     precision: 0.958069
2024-06-23 23:58:33,534 INFO     recall: 0.948817
2024-06-23 23:58:33,534 INFO     fscore: 0.953421
2024-06-23 23:59:28,108 DEBUG    epoch 23-step 100 loss: 0.489994
2024-06-24 00:00:18,743 DEBUG    epoch 23-step 200 loss: 0.460852
2024-06-24 00:01:09,313 DEBUG    epoch 23-step 300 loss: 0.467471
2024-06-24 00:01:57,383 DEBUG    epoch 23-step 400 loss: 0.498905
2024-06-24 00:02:46,634 DEBUG    epoch 23-step 500 loss: 0.509305
2024-06-24 00:03:37,344 DEBUG    epoch 23-step 600 loss: 0.476211
2024-06-24 00:03:55,767 INFO     precision: 0.946788
2024-06-24 00:03:55,768 INFO     recall: 0.953646
2024-06-24 00:03:55,768 INFO     fscore: 0.950204
2024-06-24 00:04:44,006 DEBUG    epoch 24-step 100 loss: 0.455584
2024-06-24 00:05:36,662 DEBUG    epoch 24-step 200 loss: 0.423647
2024-06-24 00:06:29,891 DEBUG    epoch 24-step 300 loss: 0.440500
2024-06-24 00:07:22,400 DEBUG    epoch 24-step 400 loss: 0.453033
2024-06-24 00:08:15,074 DEBUG    epoch 24-step 500 loss: 0.450056
2024-06-24 00:09:06,446 DEBUG    epoch 24-step 600 loss: 0.477172
2024-06-24 00:09:21,684 INFO     precision: 0.960784
2024-06-24 00:09:21,685 INFO     recall: 0.946403
2024-06-24 00:09:21,685 INFO     fscore: 0.953539
2024-06-24 00:10:13,521 DEBUG    epoch 25-step 100 loss: 0.425519
2024-06-24 00:11:05,345 DEBUG    epoch 25-step 200 loss: 0.437648
2024-06-24 00:12:01,738 DEBUG    epoch 25-step 300 loss: 0.433636
2024-06-24 00:12:50,969 DEBUG    epoch 25-step 400 loss: 0.439156
2024-06-24 00:13:40,208 DEBUG    epoch 25-step 500 loss: 0.465627
2024-06-24 00:14:32,907 DEBUG    epoch 25-step 600 loss: 0.469987
2024-06-24 00:14:49,554 INFO     precision: 0.958637
2024-06-24 00:14:49,555 INFO     recall: 0.951231
2024-06-24 00:14:49,555 INFO     fscore: 0.954920
2024-06-24 00:15:42,562 DEBUG    epoch 26-step 100 loss: 0.428048
2024-06-24 00:16:33,636 DEBUG    epoch 26-step 200 loss: 0.405949
2024-06-24 00:17:25,652 DEBUG    epoch 26-step 300 loss: 0.432576
2024-06-24 00:18:17,676 DEBUG    epoch 26-step 400 loss: 0.405845
2024-06-24 00:19:09,574 DEBUG    epoch 26-step 500 loss: 0.425204
2024-06-24 00:20:05,785 DEBUG    epoch 26-step 600 loss: 0.399694
2024-06-24 00:20:20,522 INFO     precision: 0.953110
2024-06-24 00:20:20,523 INFO     recall: 0.961854
2024-06-24 00:20:20,523 INFO     fscore: 0.957462
2024-06-24 00:21:18,445 DEBUG    epoch 27-step 100 loss: 0.369370
2024-06-24 00:22:09,727 DEBUG    epoch 27-step 200 loss: 0.388054
2024-06-24 00:23:01,571 DEBUG    epoch 27-step 300 loss: 0.393435
2024-06-24 00:23:45,811 DEBUG    epoch 27-step 400 loss: 0.383844
2024-06-24 00:24:34,872 DEBUG    epoch 27-step 500 loss: 0.368536
2024-06-24 00:25:29,537 DEBUG    epoch 27-step 600 loss: 0.397811
2024-06-24 00:25:45,426 INFO     precision: 0.959203
2024-06-24 00:25:45,427 INFO     recall: 0.953646
2024-06-24 00:25:45,427 INFO     fscore: 0.956416
