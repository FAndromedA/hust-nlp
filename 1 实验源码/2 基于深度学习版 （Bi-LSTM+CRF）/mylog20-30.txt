2024-06-23 11:44:31,949 INFO     model has been loaded from ./mySave/model_epoch20.pkl
2024-06-23 11:44:31,991 DEBUG    bert.embeddings.word_embeddings.weight: torch.Size([21421, 768]), require_grad=False
2024-06-23 11:44:31,991 DEBUG    bert.embeddings.position_embeddings.weight: torch.Size([512, 768]), require_grad=False
2024-06-23 11:44:31,991 DEBUG    bert.embeddings.token_type_embeddings.weight: torch.Size([2, 768]), require_grad=False
2024-06-23 11:44:31,991 DEBUG    bert.embeddings.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,991 DEBUG    bert.embeddings.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,991 DEBUG    bert.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,991 DEBUG    bert.encoder.layer.0.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,991 DEBUG    bert.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.0.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.1.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.1.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.1.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,992 DEBUG    bert.encoder.layer.1.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.1.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.1.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.1.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.1.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.1.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:31,993 DEBUG    bert.encoder.layer.2.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.2.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.2.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.2.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.3.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,994 DEBUG    bert.encoder.layer.4.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.4.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.5.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.5.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.5.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,995 DEBUG    bert.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.5.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.5.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.5.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.5.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.5.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.5.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:31,996 DEBUG    bert.encoder.layer.6.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.6.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.6.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.6.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.6.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.7.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,997 DEBUG    bert.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.8.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.9.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.9.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.9.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,998 DEBUG    bert.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.9.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.9.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.9.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.9.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.9.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.9.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:31,999 DEBUG    bert.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.10.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.10.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.10.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.10.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.10.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.attention.self.query.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.attention.self.key.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.attention.self.value.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.attention.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.intermediate.dense.bias: torch.Size([3072]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.output.dense.weight: torch.Size([768, 3072]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.output.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.output.LayerNorm.weight: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,000 DEBUG    bert.encoder.layer.11.output.LayerNorm.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,001 DEBUG    bert.pooler.dense.weight: torch.Size([768, 768]), require_grad=False
2024-06-23 11:44:32,001 DEBUG    bert.pooler.dense.bias: torch.Size([768]), require_grad=False
2024-06-23 11:44:32,001 DEBUG    lstm.weight_ih_l0: torch.Size([512, 768]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    lstm.weight_hh_l0: torch.Size([512, 128]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    lstm.bias_ih_l0: torch.Size([512]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    lstm.bias_hh_l0: torch.Size([512]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    lstm.weight_ih_l0_reverse: torch.Size([512, 768]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    lstm.weight_hh_l0_reverse: torch.Size([512, 128]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    lstm.bias_ih_l0_reverse: torch.Size([512]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    lstm.bias_hh_l0_reverse: torch.Size([512]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    residual_projection.weight: torch.Size([256, 256]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    residual_projection.bias: torch.Size([256]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    BN.weight: torch.Size([256]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    BN.bias: torch.Size([256]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    hidden2tag.weight: torch.Size([4, 256]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    hidden2tag.bias: torch.Size([4]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    crf.start_transitions: torch.Size([4]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    crf.end_transitions: torch.Size([4]), require_grad=True
2024-06-23 11:44:32,001 DEBUG    crf.transitions: torch.Size([4, 4]), require_grad=True
/home/vipuser/miniconda3/envs/nlplab/lib/python3.10/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
2024-06-23 11:45:24,825 DEBUG    epoch 21-step 100 loss: 12.708775
2024-06-23 11:46:17,133 DEBUG    epoch 21-step 200 loss: 11.564953
2024-06-23 11:47:12,749 DEBUG    epoch 21-step 300 loss: 11.226141
2024-06-23 11:48:09,330 DEBUG    epoch 21-step 400 loss: 11.172493
2024-06-23 11:49:08,734 DEBUG    epoch 21-step 500 loss: 11.038928
2024-06-23 11:50:05,639 DEBUG    epoch 21-step 600 loss: 10.857955
2024-06-23 11:50:15,521 INFO     precision: 0.904813
2024-06-23 11:50:15,521 INFO     recall: 0.881294
2024-06-23 11:50:15,521 INFO     fscore: 0.892899
2024-06-23 11:51:13,366 DEBUG    epoch 22-step 100 loss: 10.794980
2024-06-23 11:52:08,274 DEBUG    epoch 22-step 200 loss: 10.683744
2024-06-23 11:52:59,228 DEBUG    epoch 22-step 300 loss: 10.554482
2024-06-23 11:53:53,411 DEBUG    epoch 22-step 400 loss: 10.451431
2024-06-23 11:54:52,070 DEBUG    epoch 22-step 500 loss: 10.466881
2024-06-23 11:55:50,622 DEBUG    epoch 22-step 600 loss: 10.537854
2024-06-23 11:56:01,323 INFO     precision: 0.906979
2024-06-23 11:56:01,324 INFO     recall: 0.883405
2024-06-23 11:56:01,324 INFO     fscore: 0.895037
2024-06-23 11:56:50,663 DEBUG    epoch 23-step 100 loss: 10.351929
2024-06-23 11:57:50,499 DEBUG    epoch 23-step 200 loss: 10.368242
2024-06-23 11:58:44,735 DEBUG    epoch 23-step 300 loss: 10.310786
2024-06-23 11:59:40,066 DEBUG    epoch 23-step 400 loss: 10.305101
2024-06-23 12:00:34,099 DEBUG    epoch 23-step 500 loss: 10.142717
2024-06-23 12:01:32,424 DEBUG    epoch 23-step 600 loss: 10.182970
2024-06-23 12:01:42,674 INFO     precision: 0.907742
2024-06-23 12:01:42,675 INFO     recall: 0.885997
2024-06-23 12:01:42,675 INFO     fscore: 0.896738
2024-06-23 12:02:36,934 DEBUG    epoch 24-step 100 loss: 10.085103
2024-06-23 12:03:29,452 DEBUG    epoch 24-step 200 loss: 10.164979
2024-06-23 12:04:22,770 DEBUG    epoch 24-step 300 loss: 10.140502
2024-06-23 12:05:19,957 DEBUG    epoch 24-step 400 loss: 9.992031
2024-06-23 12:06:16,707 DEBUG    epoch 24-step 500 loss: 9.931563
2024-06-23 12:07:16,074 DEBUG    epoch 24-step 600 loss: 9.931279
2024-06-23 12:07:28,058 INFO     precision: 0.908967
2024-06-23 12:07:28,058 INFO     recall: 0.887663
2024-06-23 12:07:28,058 INFO     fscore: 0.898189
2024-06-23 12:08:23,907 DEBUG    epoch 25-step 100 loss: 10.007259
2024-06-23 12:09:20,811 DEBUG    epoch 25-step 200 loss: 9.848570
2024-06-23 12:10:20,148 DEBUG    epoch 25-step 300 loss: 9.883274
2024-06-23 12:11:16,071 DEBUG    epoch 25-step 400 loss: 9.788076
2024-06-23 12:12:10,430 DEBUG    epoch 25-step 500 loss: 9.781890
2024-06-23 12:13:06,248 DEBUG    epoch 25-step 600 loss: 9.786593
2024-06-23 12:13:18,362 INFO     precision: 0.910505
2024-06-23 12:13:18,363 INFO     recall: 0.890514
2024-06-23 12:13:18,363 INFO     fscore: 0.900399
2024-06-23 12:14:14,814 DEBUG    epoch 26-step 100 loss: 9.760218
2024-06-23 12:15:10,412 DEBUG    epoch 26-step 200 loss: 9.629369
2024-06-23 12:16:07,343 DEBUG    epoch 26-step 300 loss: 9.668096
2024-06-23 12:17:06,546 DEBUG    epoch 26-step 400 loss: 9.673588
2024-06-23 12:17:58,619 DEBUG    epoch 26-step 500 loss: 9.698276
2024-06-23 12:18:49,101 DEBUG    epoch 26-step 600 loss: 9.599845
2024-06-23 12:18:59,330 INFO     precision: 0.912170
2024-06-23 12:18:59,330 INFO     recall: 0.890588
2024-06-23 12:18:59,330 INFO     fscore: 0.901250
2024-06-23 12:19:59,496 DEBUG    epoch 27-step 100 loss: 9.482924
2024-06-23 12:20:58,182 DEBUG    epoch 27-step 200 loss: 9.559558
2024-06-23 12:21:53,805 DEBUG    epoch 27-step 300 loss: 9.463324
2024-06-23 12:22:45,418 DEBUG    epoch 27-step 400 loss: 9.460352
2024-06-23 12:23:38,530 DEBUG    epoch 27-step 500 loss: 9.465067
2024-06-23 12:24:33,762 DEBUG    epoch 27-step 600 loss: 9.459944
2024-06-23 12:24:45,496 INFO     precision: 0.912470
2024-06-23 12:24:45,496 INFO     recall: 0.893550
2024-06-23 12:24:45,496 INFO     fscore: 0.902911
2024-06-23 12:25:45,038 DEBUG    epoch 28-step 100 loss: 9.327704
2024-06-23 12:26:39,338 DEBUG    epoch 28-step 200 loss: 9.383136
2024-06-23 12:27:30,518 DEBUG    epoch 28-step 300 loss: 9.338127
2024-06-23 12:28:27,261 DEBUG    epoch 28-step 400 loss: 9.324788
2024-06-23 12:29:26,050 DEBUG    epoch 28-step 500 loss: 9.300637
2024-06-23 12:30:20,811 DEBUG    epoch 28-step 600 loss: 9.347357
2024-06-23 12:30:31,529 INFO     precision: 0.911422
2024-06-23 12:30:31,529 INFO     recall: 0.893772
2024-06-23 12:30:31,529 INFO     fscore: 0.902511
2024-06-23 12:31:29,985 DEBUG    epoch 29-step 100 loss: 9.269729
2024-06-23 12:32:24,072 DEBUG    epoch 29-step 200 loss: 9.229426
2024-06-23 12:33:16,101 DEBUG    epoch 29-step 300 loss: 9.122599
2024-06-23 12:34:09,742 DEBUG    epoch 29-step 400 loss: 9.123105
2024-06-23 12:35:05,112 DEBUG    epoch 29-step 500 loss: 9.047323
2024-06-23 12:36:02,926 DEBUG    epoch 29-step 600 loss: 9.195007
2024-06-23 12:36:12,542 INFO     precision: 0.914259
2024-06-23 12:36:12,542 INFO     recall: 0.891477
2024-06-23 12:36:12,542 INFO     fscore: 0.902724
2024-06-23 12:36:13,445 INFO     model has been saved in  ./mySave/model_epoch29.pkl
